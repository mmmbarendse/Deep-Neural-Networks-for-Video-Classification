{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:14:17.264442Z",
     "start_time": "2020-05-14T17:14:17.250090Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# whether to log each feature and sequence status\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:14:18.583218Z",
     "start_time": "2020-05-14T17:14:17.657048Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-02 23:44:41.529653: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-02 23:44:41.689892: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /app/lib\n",
      "2023-04-02 23:44:41.689910: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-02 23:44:41.719374: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-02 23:44:42.276326: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /app/lib\n",
      "2023-04-02 23:44:42.276398: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /app/lib\n",
      "2023-04-02 23:44:42.276406: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "from shutil import rmtree\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:14:18.993048Z",
     "start_time": "2020-05-14T17:14:18.988996Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__file__ = os.path.abspath('helper_precomputing_CNN_features.ipynb')\n",
    "DNN_lib_path = Path(__file__).parents[1].__str__()\n",
    "path_data = DNN_lib_path + '/data_cnn_ts_3d/'\n",
    "path_cache = DNN_lib_path + '/cache/'\n",
    "\n",
    "custom_model_name = 'ResNet50_test'\n",
    "path_features = path_cache + '/features/' + custom_model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:14:19.880582Z",
     "start_time": "2020-05-14T17:14:19.867630Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup logging\n",
    "# any explicit log messages or uncaught errors to stdout and file /logs.log\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(f\"{DNN_lib_path}/logs_{custom_model_name}_training.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "# init logger\n",
    "logger = logging.getLogger()\n",
    "# make logger aware of any uncaught exceptions\n",
    "def handle_exception(exc_type, exc_value, exc_traceback):\n",
    "    if issubclass(exc_type, KeyboardInterrupt):\n",
    "        sys.__excepthook__(exc_type, exc_value, exc_traceback)\n",
    "        return\n",
    "\n",
    "    logger.error(\"Uncaught exception\", exc_info=(exc_type, exc_value, exc_traceback))\n",
    "sys.excepthook = handle_exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:14:24.947193Z",
     "start_time": "2020-05-14T17:14:20.448078Z"
    }
   },
   "outputs": [],
   "source": [
    "from deepvideoclassification.architectures import Architecture\n",
    "from deepvideoclassification.data import Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>frame</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3</td>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3-0000.npy</td>\n",
       "      <td>pre-deboarding</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3</td>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3-0001.npy</td>\n",
       "      <td>pre-deboarding</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3</td>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3-0002.npy</td>\n",
       "      <td>pre-deboarding</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3</td>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3-0003.npy</td>\n",
       "      <td>pre-deboarding</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3</td>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3-0004.npy</td>\n",
       "      <td>pre-deboarding</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3</td>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3-0195.npy</td>\n",
       "      <td>post-boarding</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3</td>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3-0196.npy</td>\n",
       "      <td>post-boarding</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3</td>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3-0197.npy</td>\n",
       "      <td>post-boarding</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3</td>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3-0198.npy</td>\n",
       "      <td>post-boarding</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3</td>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3-0199.npy</td>\n",
       "      <td>post-boarding</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                video  \\\n",
       "0     trajs_2017-03-09_Ut_3062_door_3   \n",
       "1     trajs_2017-03-09_Ut_3062_door_3   \n",
       "2     trajs_2017-03-09_Ut_3062_door_3   \n",
       "3     trajs_2017-03-09_Ut_3062_door_3   \n",
       "4     trajs_2017-03-09_Ut_3062_door_3   \n",
       "...                               ...   \n",
       "5995  trajs_2018-05-30_Ut_3058_door_3   \n",
       "5996  trajs_2018-05-30_Ut_3058_door_3   \n",
       "5997  trajs_2018-05-30_Ut_3058_door_3   \n",
       "5998  trajs_2018-05-30_Ut_3058_door_3   \n",
       "5999  trajs_2018-05-30_Ut_3058_door_3   \n",
       "\n",
       "                                         frame           label  split  \n",
       "0     trajs_2017-03-09_Ut_3062_door_3-0000.npy  pre-deboarding   test  \n",
       "1     trajs_2017-03-09_Ut_3062_door_3-0001.npy  pre-deboarding   test  \n",
       "2     trajs_2017-03-09_Ut_3062_door_3-0002.npy  pre-deboarding   test  \n",
       "3     trajs_2017-03-09_Ut_3062_door_3-0003.npy  pre-deboarding   test  \n",
       "4     trajs_2017-03-09_Ut_3062_door_3-0004.npy  pre-deboarding   test  \n",
       "...                                        ...             ...    ...  \n",
       "5995  trajs_2018-05-30_Ut_3058_door_3-0195.npy   post-boarding  valid  \n",
       "5996  trajs_2018-05-30_Ut_3058_door_3-0196.npy   post-boarding  valid  \n",
       "5997  trajs_2018-05-30_Ut_3058_door_3-0197.npy   post-boarding  valid  \n",
       "5998  trajs_2018-05-30_Ut_3058_door_3-0198.npy   post-boarding  valid  \n",
       "5999  trajs_2018-05-30_Ut_3058_door_3-0199.npy   post-boarding  valid  \n",
       "\n",
       "[6000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df = pd.read_csv(path_data + 'labels.csv', usecols=['video','frame','label','split']).sort_values(['video', 'frame'])\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>frame</th>\n",
       "      <th>split</th>\n",
       "      <th>label_boarding</th>\n",
       "      <th>label_deboarding</th>\n",
       "      <th>label_phase-change</th>\n",
       "      <th>label_post-boarding</th>\n",
       "      <th>label_pre-deboarding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3</td>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3-0000.npy</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3</td>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3-0001.npy</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3</td>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3-0002.npy</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3</td>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3-0003.npy</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3</td>\n",
       "      <td>trajs_2017-03-09_Ut_3062_door_3-0004.npy</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3</td>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3-0195.npy</td>\n",
       "      <td>valid</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3</td>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3-0196.npy</td>\n",
       "      <td>valid</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3</td>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3-0197.npy</td>\n",
       "      <td>valid</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3</td>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3-0198.npy</td>\n",
       "      <td>valid</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3</td>\n",
       "      <td>trajs_2018-05-30_Ut_3058_door_3-0199.npy</td>\n",
       "      <td>valid</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                video  \\\n",
       "0     trajs_2017-03-09_Ut_3062_door_3   \n",
       "1     trajs_2017-03-09_Ut_3062_door_3   \n",
       "2     trajs_2017-03-09_Ut_3062_door_3   \n",
       "3     trajs_2017-03-09_Ut_3062_door_3   \n",
       "4     trajs_2017-03-09_Ut_3062_door_3   \n",
       "...                               ...   \n",
       "5995  trajs_2018-05-30_Ut_3058_door_3   \n",
       "5996  trajs_2018-05-30_Ut_3058_door_3   \n",
       "5997  trajs_2018-05-30_Ut_3058_door_3   \n",
       "5998  trajs_2018-05-30_Ut_3058_door_3   \n",
       "5999  trajs_2018-05-30_Ut_3058_door_3   \n",
       "\n",
       "                                         frame  split  label_boarding  \\\n",
       "0     trajs_2017-03-09_Ut_3062_door_3-0000.npy   test               0   \n",
       "1     trajs_2017-03-09_Ut_3062_door_3-0001.npy   test               0   \n",
       "2     trajs_2017-03-09_Ut_3062_door_3-0002.npy   test               0   \n",
       "3     trajs_2017-03-09_Ut_3062_door_3-0003.npy   test               0   \n",
       "4     trajs_2017-03-09_Ut_3062_door_3-0004.npy   test               0   \n",
       "...                                        ...    ...             ...   \n",
       "5995  trajs_2018-05-30_Ut_3058_door_3-0195.npy  valid               0   \n",
       "5996  trajs_2018-05-30_Ut_3058_door_3-0196.npy  valid               0   \n",
       "5997  trajs_2018-05-30_Ut_3058_door_3-0197.npy  valid               0   \n",
       "5998  trajs_2018-05-30_Ut_3058_door_3-0198.npy  valid               0   \n",
       "5999  trajs_2018-05-30_Ut_3058_door_3-0199.npy  valid               0   \n",
       "\n",
       "      label_deboarding  label_phase-change  label_post-boarding  \\\n",
       "0                    0                   0                    0   \n",
       "1                    0                   0                    0   \n",
       "2                    0                   0                    0   \n",
       "3                    0                   0                    0   \n",
       "4                    0                   0                    0   \n",
       "...                ...                 ...                  ...   \n",
       "5995                 0                   0                    1   \n",
       "5996                 0                   0                    1   \n",
       "5997                 0                   0                    1   \n",
       "5998                 0                   0                    1   \n",
       "5999                 0                   0                    1   \n",
       "\n",
       "      label_pre-deboarding  \n",
       "0                        1  \n",
       "1                        1  \n",
       "2                        1  \n",
       "3                        1  \n",
       "4                        1  \n",
       "...                    ...  \n",
       "5995                     0  \n",
       "5996                     0  \n",
       "5997                     0  \n",
       "5998                     0  \n",
       "5999                     0  \n",
       "\n",
       "[6000 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dummied = pd.get_dummies(label_df, columns=['label'])\n",
    "labels_dummied \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerization_dict = dict({\n",
    "    None : 4,\n",
    "    np.nan: 4, \n",
    "    'pre-deboarding': 0,\n",
    "    'deboarding': 1,\n",
    "    'phase-change': 2,\n",
    "    'boarding': 3,\n",
    "    'post-boarding': 0\n",
    "})\n",
    "\n",
    "class Data(object):\n",
    "\n",
    "    def __init__(self, sequence_length,\n",
    "                 return_CNN_features=True,\n",
    "                 pretrained_model_name=None,\n",
    "                 pooling=None,\n",
    "                 frame_size=None,\n",
    "                 custom_model_name=None,\n",
    "                 _bed=False,\n",
    "                 verbose=True,\n",
    "                 return_generator = False):\n",
    "        \"\"\"\n",
    "        Data object constructor\n",
    "        \n",
    "        \n",
    "        :sequence_length: number of frames in sequence to be returned by Data object\n",
    "        :return_CNN_features: whether to return precomputed features or return frames (or sequences of features/frames if sequence_length>1)\n",
    "\n",
    "        :return_features: if True then return features (or sequences of feature) from pretrained model, if False then return frames (or sequences of frames)        \n",
    "        :pretrained_model_name: name of pretrained model (or None if not using pretrained model e.g. for 3D-CNN)\n",
    "        :pooling: name of pooling variant (or None if not using pretrained model e.g. for 3D-CNN)\n",
    "        :frame_size: size that frames are resized to (this is looked up for pretrained models)\n",
    "        :aug3mentation: whether to apply data augmentation (horizontal flips)\n",
    "        :oversampling: whether to apply oversampling to create class balance\n",
    "        \n",
    "        :model_weights_path: path to custom model weights if we want to load CNN model we've fine-tuned to produce features (e.g. for LRCNN)\n",
    "        :custom_model_name: custom output name to append to pretrained model name\n",
    "        \n",
    "        :return_generator: if True and sequence_length > 1 and return_CNN_features == False, then do not return dataset, instead construct h5 file with sequences for each split and return generator that samples from that (dataset of sequecne frames too big to load into memory)\n",
    "        :batch_size: size of batches that generator must return\n",
    "        \n",
    "        :verbose: whether to log details\n",
    "        \n",
    "        Notes: \n",
    "        * if pretrained_model_name != None and return_CNN_features=False then will first apply preprocessor to frames (or frame sequences)\n",
    "        * if return_generator = True and sequence_length > 1 and return_CNN_features == False, large h5 files will be created in cache before returning generator\n",
    "        \"\"\"\n",
    "\n",
    "        # required params\n",
    "        self.sequence_length = sequence_length\n",
    "        self.frame_size = frame_size\n",
    "\n",
    "        # optional params\n",
    "        self.pretrained_model_name = pretrained_model_name\n",
    "        self.pooling = pooling\n",
    "        self.return_CNN_features = return_CNN_features\n",
    "        self.custom_model_name = custom_model_name\n",
    "        self.return_generator = return_generator\n",
    "\n",
    "        self.bed = _bed\n",
    "        self.frame_size = frame_size\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.x_train = []\n",
    "        self.y_train = []\n",
    "        #\n",
    "        self.x_valid = []\n",
    "        self.y_valid = []\n",
    "        #\n",
    "        self.x_test = []\n",
    "        self.y_test = []\n",
    "\n",
    "        ################\n",
    "        ### Prepare data\n",
    "        ################\n",
    "\n",
    "        #Label loading\n",
    "        self.labels = pd.read_csv(path_data + 'labels.csv', usecols=['video','frame','label','split']).sort_values(['video', 'frame'])\n",
    "        \n",
    "        # get label columns list and build label map dict\n",
    "        label_columns = []\n",
    "        label_map = {}\n",
    "        label_map_idx = 0\n",
    "        for i, col in enumerate(labels_dummied.columns):\n",
    "            if col[:6] == 'label_':\n",
    "                label_columns.append(col)\n",
    "                label_map[label_map_idx] = col\n",
    "                label_map_idx+=1\n",
    "\n",
    "        self.label_map = label_map\n",
    "\n",
    "        # get video paths\n",
    "        self.path_videos = [f\"{path_data}/{video}\" for video in os.listdir(path_data) if os.path.isdir(f\"{path_data}/{video}\")]\n",
    "\n",
    "        # check that there is 1 frame file for each label file and raise error if they don't match\n",
    "        path_frames = []\n",
    "        for folder, subs, files in os.walk(path_data):\n",
    "            for filename in files:\n",
    "                if self.bed & (filename[-4:].lower() == '.npy'):\n",
    "                    path_frames.append(os.path.abspath(\n",
    "                        os.path.join(folder, filename)))\n",
    "\n",
    "                if filename[-4:].lower() == '.jpg' or filename[-4:].lower() == 'jpeg' or filename[-4:].lower() == '.png':\n",
    "                    path_frames.append(os.path.abspath(\n",
    "                        os.path.join(folder, filename)))\n",
    "\n",
    "        if len(path_frames) != len(self.labels):\n",
    "            error_msg = 'IMPORTANT ERROR: Number of frames ({}) in /data/ video folders needs to match number of labels ({}) in labels.csv - use notebooks/helper_check_frames_against_labels.ipynb to investigate... Note, only labels.csv and the frames you want to use (in video subfolders) should be in /data/'.format(\n",
    "                len(path_frames), len(self.labels))\n",
    "            logger.info(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        # pull number of classes from labels shape\n",
    "        self.num_classes = self.labels['label'].nunique()\n",
    "\n",
    "        # create dict mapping video to train/valid/test split assignment\n",
    "        video_splits = self.labels[['video', 'split']].drop_duplicates()\n",
    "        video_splits.set_index(\"video\", inplace=True)\n",
    "        video_splits = video_splits.to_dict()['split']\n",
    "        self.video_splits = video_splits\n",
    "\n",
    "        # precompute resized frames (won't recompute if already resized)\n",
    "        #resize_frames(self.frame_size, _bed = self.bed, _verbose = self.verbose)\n",
    "\n",
    "        ###################################\n",
    "        ### load features / build sequences\n",
    "        ###################################\n",
    "\n",
    "        # load features/frames from all videos and concat into big array for each of train, valid and test\n",
    "        assert self.return_CNN_features\n",
    "\n",
    "        if verbose:\n",
    "            logging.info(\n",
    "                \"Loading features sequence data into memory [may take a few minutes]\")\n",
    "\n",
    "        #####################\n",
    "        ### feature sequences\n",
    "        #####################\n",
    "\n",
    "        path_features = path_cache + 'features/' + self.custom_model_name\n",
    "        path_labels = path_cache + 'labels/'\n",
    "        \n",
    "        bes_names = [be for be in os.listdir(path_data) if os.path.isdir(f\"{path_data}/{be}\")]\n",
    "\n",
    "        # loop over all vids and load precomputed features into memory as sequences\n",
    "        for c, be_name in enumerate(bes_names):\n",
    "\n",
    "            path_be = f'{path_data}/{be_name}'\n",
    "\n",
    "            if verbose:\n",
    "                logging.info(\"Loading features sequence data into memory {}/{}\".format(c+1,len(path_be)))\n",
    "\n",
    "            ### create sequence: features\n",
    "            # load precomputed features\n",
    "            features = np.load(f\"{path_features}/{be_name}.npy\")\n",
    "            # build sequences\n",
    "            x = []\n",
    "            for i in range(self.sequence_length, len(features) + 1):\n",
    "                x.append(features[i-self.sequence_length:i])\n",
    "            x = np.array(x)\n",
    "            \n",
    "\n",
    "            # temp lists to store sequences\n",
    "            be_labels = self.labels[self.labels.video == be_name]\n",
    "            y = []\n",
    "            for i in range(self.sequence_length, len(be_labels) + 1):\n",
    "                label = be_labels.label.iloc[i-1]\n",
    "                if (label is None) or (label == np.nan):\n",
    "                    label = 'nan'\n",
    "                y.append(label)\n",
    "            y = np.array(list(map(numerization_dict.get, y)))\n",
    "            y = to_categorical(y, num_classes=5)\n",
    "\n",
    "            assert len(x) == len(y), f'Length of features ({len(x)}) does not match length of labels ({len(y)})'\n",
    "\n",
    "            ### build output\n",
    "            if self.video_splits[be_name] == \"train\":\n",
    "                self.x_train.append(x)\n",
    "                self.y_train.append(y)\n",
    "            if self.video_splits[be_name] == \"valid\":\n",
    "                self.x_valid.append(x)\n",
    "                self.y_valid.append(y)\n",
    "            if self.video_splits[be_name] == \"test\":\n",
    "                self.x_test.append(x)\n",
    "                self.y_test.append(y)\n",
    "\n",
    "        #################################\n",
    "        ### get file paths for each split\n",
    "        #################################\n",
    "        #\n",
    "        # Note: only makes sense for sequence_length = 1\n",
    "\n",
    "        # get file paths: train\n",
    "        dflab = self.labels[self.labels['split'] == 'train']\n",
    "        self.paths_train = list(\n",
    "            path_data + dflab['video'] + \"/\" + dflab['frame'])\n",
    "\n",
    "        # get file paths: valid\n",
    "        dflab = self.labels[self.labels['split'] == 'valid']\n",
    "        self.paths_valid = list(\n",
    "            path_data + dflab['video'] + \"/\" + dflab['frame'])\n",
    "\n",
    "        # get file paths: test\n",
    "        dflab = self.labels[self.labels['split'] == 'test']\n",
    "        self.paths_test = list(\n",
    "            path_data + dflab['video'] + \"/\" + dflab['frame'])\n",
    "\n",
    "        #################################################\n",
    "        ### reshape list outputs (if not using generator)\n",
    "        #################################################\n",
    "\n",
    "        ## e.g. (9846, 224, 224, 3) for frames [return_CNN_features=True]\n",
    "        ## or  (9846, 512) for features [return_CNN_features=False]\n",
    "        self.x_train = np.concatenate(self.x_train, axis=0)\n",
    "        self.y_train = np.concatenate(self.y_train, axis=0)\n",
    "        self.x_valid = np.concatenate(self.x_valid, axis=0)\n",
    "        self.y_valid = np.concatenate(self.y_valid, axis=0)\n",
    "        self.x_test = np.concatenate(self.x_test, axis=0)\n",
    "        self.y_test = np.concatenate(self.y_test, axis=0)\n",
    "\n",
    "        self.total_rows_train = len(self.x_train)\n",
    "        self.total_rows_valid = len(self.x_valid)\n",
    "        self.total_rows_test = len(self.x_test)\n",
    "\n",
    "        # shuffle train and validation set\n",
    "        self.x_train, self.y_train = shuffle(self.x_train, self.y_train)\n",
    "        self.x_valid, self.y_valid = shuffle(self.x_valid, self.y_valid)\n",
    "\n",
    "        # update progress\n",
    "        if self.verbose:\n",
    "            print(\"Done initializing data with #samples: train={}, valid={}, test={}\".format(\n",
    "                self.total_rows_train, self.total_rows_valid, self.total_rows_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = {\n",
    "    'architecture': 'video_mlp_concat',\n",
    "    'dropout': 0.2,\n",
    "    'layer_1_size': 256,\n",
    "    'layer_2_size': 512,\n",
    "    'layer_3_size': 256,\n",
    "    'model_id': 1,\n",
    "    'pooling': 'max',\n",
    "    'pretrained_model_name': 'SimpleRNN',\n",
    "    'custom_model_name': custom_model_name,\n",
    "    'path_features': f'/cache/{custom_model_name}',\n",
    "    'sequence_length': 20,\n",
    "    'sequence_model': \"LSTM\",\n",
    "    'sequence_model_layers': 3,\n",
    "    'frame_size': (32, 32)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:15:28.835943Z",
     "start_time": "2020-05-14T17:14:48.938251Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-02 23:51:51,801 [MainThread  ] [INFO ]  Loading features sequence data into memory [may take a few minutes]\n",
      "2023-04-02 23:51:51,803 [MainThread  ] [INFO ]  Loading features sequence data into memory 1/139\n",
      "2023-04-02 23:51:51,814 [MainThread  ] [INFO ]  Loading features sequence data into memory 2/139\n",
      "2023-04-02 23:51:51,825 [MainThread  ] [INFO ]  Loading features sequence data into memory 3/139\n",
      "2023-04-02 23:51:51,836 [MainThread  ] [INFO ]  Loading features sequence data into memory 4/139\n",
      "2023-04-02 23:51:51,846 [MainThread  ] [INFO ]  Loading features sequence data into memory 5/139\n",
      "2023-04-02 23:51:51,856 [MainThread  ] [INFO ]  Loading features sequence data into memory 6/139\n",
      "2023-04-02 23:51:51,883 [MainThread  ] [INFO ]  Loading features sequence data into memory 7/139\n",
      "2023-04-02 23:51:51,947 [MainThread  ] [INFO ]  Loading features sequence data into memory 8/139\n",
      "2023-04-02 23:51:52,012 [MainThread  ] [INFO ]  Loading features sequence data into memory 9/139\n",
      "2023-04-02 23:51:52,032 [MainThread  ] [INFO ]  Loading features sequence data into memory 10/139\n",
      "2023-04-02 23:51:52,054 [MainThread  ] [INFO ]  Loading features sequence data into memory 11/139\n",
      "2023-04-02 23:51:52,094 [MainThread  ] [INFO ]  Loading features sequence data into memory 12/139\n",
      "2023-04-02 23:51:52,110 [MainThread  ] [INFO ]  Loading features sequence data into memory 13/139\n",
      "2023-04-02 23:51:52,126 [MainThread  ] [INFO ]  Loading features sequence data into memory 14/139\n",
      "2023-04-02 23:51:52,149 [MainThread  ] [INFO ]  Loading features sequence data into memory 15/139\n",
      "2023-04-02 23:51:52,164 [MainThread  ] [INFO ]  Loading features sequence data into memory 16/139\n",
      "2023-04-02 23:51:52,181 [MainThread  ] [INFO ]  Loading features sequence data into memory 17/139\n",
      "2023-04-02 23:51:52,204 [MainThread  ] [INFO ]  Loading features sequence data into memory 18/139\n",
      "2023-04-02 23:51:52,218 [MainThread  ] [INFO ]  Loading features sequence data into memory 19/139\n",
      "2023-04-02 23:51:52,233 [MainThread  ] [INFO ]  Loading features sequence data into memory 20/139\n",
      "2023-04-02 23:51:52,253 [MainThread  ] [INFO ]  Loading features sequence data into memory 21/139\n",
      "2023-04-02 23:51:52,269 [MainThread  ] [INFO ]  Loading features sequence data into memory 22/139\n",
      "2023-04-02 23:51:52,289 [MainThread  ] [INFO ]  Loading features sequence data into memory 23/139\n",
      "2023-04-02 23:51:52,321 [MainThread  ] [INFO ]  Loading features sequence data into memory 24/139\n",
      "2023-04-02 23:51:52,373 [MainThread  ] [INFO ]  Loading features sequence data into memory 25/139\n",
      "2023-04-02 23:51:52,423 [MainThread  ] [INFO ]  Loading features sequence data into memory 26/139\n",
      "2023-04-02 23:51:52,485 [MainThread  ] [INFO ]  Loading features sequence data into memory 27/139\n",
      "2023-04-02 23:51:52,546 [MainThread  ] [INFO ]  Loading features sequence data into memory 28/139\n",
      "2023-04-02 23:51:52,806 [MainThread  ] [INFO ]  Loading features sequence data into memory 29/139\n",
      "2023-04-02 23:51:52,960 [MainThread  ] [INFO ]  Loading features sequence data into memory 30/139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done initializing data with #samples: train=362, valid=724, test=4344\n"
     ]
    }
   ],
   "source": [
    "be_data = Data(\n",
    "    sequence_length = experiment['sequence_length'],\n",
    "    return_CNN_features = True, \n",
    "    pretrained_model_name = experiment['pretrained_model_name'],\n",
    "    pooling = experiment['pooling'],\n",
    "    frame_size = experiment['frame_size'],\n",
    "    custom_model_name= experiment['custom_model_name'],\n",
    "    return_generator = False,\n",
    "    _bed = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete existing results\n",
    "if os.path.exists(DNN_lib_path + '/models/' + str(experiment[\"model_id\"]) + '/results.json'):\n",
    "    rmtree(DNN_lib_path + '/models/' + str(experiment[\"model_id\"]) + '/')\n",
    "# create models folder if doesn't exist\n",
    "if not os.path.exists(DNN_lib_path + '/models/'):\n",
    "    os.makedirs(DNN_lib_path + '/models/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "{'architecture': 'video_mlp_concat', 'dropout': 0.2, 'layer_1_size': 256, 'layer_2_size': 512, 'layer_3_size': 256, 'model_id': 1, 'pooling': 'max', 'pretrained_model_name': 'SimpleRNN', 'custom_model_name': 'ResNet50_test', 'path_features': '/cache/ResNet50_test', 'sequence_length': 20, 'sequence_model': 'LSTM', 'sequence_model_layers': 3, 'frame_size': (32, 32)}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'SimpleRNN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mstr\u001b[39m(experiment[\u001b[39m\"\u001b[39m\u001b[39mmodel_id\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m   \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m60\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(experiment)\n\u001b[0;32m----> 4\u001b[0m architecture \u001b[39m=\u001b[39m Architecture(model_id \u001b[39m=\u001b[39;49m experiment[\u001b[39m'\u001b[39;49m\u001b[39mmodel_id\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m      5\u001b[0m                             architecture \u001b[39m=\u001b[39;49m experiment[\u001b[39m'\u001b[39;49m\u001b[39marchitecture\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m      6\u001b[0m                             sequence_length \u001b[39m=\u001b[39;49m experiment[\u001b[39m'\u001b[39;49m\u001b[39msequence_length\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m      7\u001b[0m                             pretrained_model_name \u001b[39m=\u001b[39;49m experiment[\u001b[39m'\u001b[39;49m\u001b[39mpretrained_model_name\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      8\u001b[0m                             custom_model_name \u001b[39m=\u001b[39;49m experiment[\u001b[39m'\u001b[39;49m\u001b[39mcustom_model_name\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      9\u001b[0m                             pooling \u001b[39m=\u001b[39;49m experiment[\u001b[39m'\u001b[39;49m\u001b[39mpooling\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     10\u001b[0m                             sequence_model \u001b[39m=\u001b[39;49m experiment[\u001b[39m'\u001b[39;49m\u001b[39msequence_model\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     11\u001b[0m                             sequence_model_layers \u001b[39m=\u001b[39;49m experiment[\u001b[39m'\u001b[39;49m\u001b[39msequence_model_layers\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     12\u001b[0m                             layer_1_size \u001b[39m=\u001b[39;49m experiment[\u001b[39m'\u001b[39;49m\u001b[39mlayer_1_size\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     13\u001b[0m                             layer_2_size \u001b[39m=\u001b[39;49m experiment[\u001b[39m'\u001b[39;49m\u001b[39mlayer_2_size\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     14\u001b[0m                             layer_3_size \u001b[39m=\u001b[39;49m experiment[\u001b[39m'\u001b[39;49m\u001b[39mlayer_3_size\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     15\u001b[0m                             dropout \u001b[39m=\u001b[39;49m experiment[\u001b[39m'\u001b[39;49m\u001b[39mdropout\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     16\u001b[0m                             _bed \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     17\u001b[0m                             verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     18\u001b[0m                             data \u001b[39m=\u001b[39;49m be_data)\n",
      "File \u001b[0;32m~/Documents/00_Uni/22_23/pap/Deep-Neural-Networks-for-Video-Classification/notebooks/../deepvideoclassification/architectures.py:164\u001b[0m, in \u001b[0;36mArchitecture.__init__\u001b[0;34m(self, model_id, architecture, sequence_length, frame_size, pretrained_model_name, custom_model_name, pooling, sequence_model, sequence_model_layers, layer_1_size, layer_2_size, layer_3_size, dropout, convolution_kernel_size, model_weights_path, batch_size, verbose, _bed, data, logger)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39m# read num features from pretrained model\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m pretrained_model_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mtype\u001b[39m(pretrained_model_name) \u001b[39m==\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_features \u001b[39m=\u001b[39m pretrained_model_len_features[pretrained_model_name]\n\u001b[1;32m    165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframe_size \u001b[39m=\u001b[39m pretrained_model_sizes[pretrained_model_name]\n\u001b[1;32m    167\u001b[0m \u001b[39m# check one of pretrained model and frame size is specified\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SimpleRNN'"
     ]
    }
   ],
   "source": [
    "print(str(experiment[\"model_id\"]) + \"   \" + \"X\"*60)\n",
    "print(experiment)\n",
    "\n",
    "architecture = Architecture(model_id = experiment['model_id'], \n",
    "                            architecture = experiment['architecture'], \n",
    "                            sequence_length = experiment['sequence_length'], \n",
    "                            pretrained_model_name = experiment['pretrained_model_name'],\n",
    "                            custom_model_name = experiment['custom_model_name'],\n",
    "                            pooling = experiment['pooling'],\n",
    "                            sequence_model = experiment['sequence_model'],\n",
    "                            sequence_model_layers = experiment['sequence_model_layers'],\n",
    "                            layer_1_size = experiment['layer_1_size'],\n",
    "                            layer_2_size = experiment['layer_2_size'],\n",
    "                            layer_3_size = experiment['layer_3_size'],\n",
    "                            dropout = experiment['dropout'],\n",
    "                            _bed = True,\n",
    "                            verbose=True,\n",
    "                            data = be_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.72514, saving model to /home/tiesbarendse/Documents/00_Uni/22_23/pap/Deep-Neural-Networks-for-Video-Classification/notebooks/../models/1/model_round_1.h5\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.72514\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.72514\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.72514 to 0.73481, saving model to /home/tiesbarendse/Documents/00_Uni/22_23/pap/Deep-Neural-Networks-for-Video-Classification/notebooks/../models/1/model_round_1.h5\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.73481\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.73481 to 0.73619, saving model to /home/tiesbarendse/Documents/00_Uni/22_23/pap/Deep-Neural-Networks-for-Video-Classification/notebooks/../models/1/model_round_1.h5\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.73619\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.73619 to 0.73757, saving model to /home/tiesbarendse/Documents/00_Uni/22_23/pap/Deep-Neural-Networks-for-Video-Classification/notebooks/../models/1/model_round_1.h5\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.73757\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.73757\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.73757\n",
      "Epoch 11: early stopping\n",
      "H1 {'loss': [0.8998665809631348, 0.5549031496047974, 0.3418158292770386, 0.2655421197414398, 0.171804741024971, 0.12567009031772614, 0.07984714210033417, 0.08068126440048218, 0.09620530903339386, 0.061531342566013336, 0.05251835286617279], 'accuracy': [0.6933701634407043, 0.7707182168960571, 0.8204419612884521, 0.8563535809516907, 0.8950276374816895, 0.90055251121521, 0.9364640712738037, 0.9337016344070435, 0.9171270728111267, 0.9530386924743652, 0.9530386924743652], 'val_loss': [0.7043179869651794, 0.5087530016899109, 0.5674331784248352, 0.5248233079910278, 0.5765919089317322, 0.8105176091194153, 1.2998802661895752, 0.7134712934494019, 1.4815360307693481, 0.9762336611747742, 1.629075527191162], 'val_accuracy': [0.7251381278038025, 0.7140883803367615, 0.7182320356369019, 0.7348066568374634, 0.7334254384040833, 0.7361878156661987, 0.7361878156661987, 0.7375690340995789, 0.7168508172035217, 0.7209944725036621, 0.730663001537323]}\n",
      "stopped_epoch1 8\n",
      "11\n",
      "0.7168508172035217\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.72514, saving model to /home/tiesbarendse/Documents/00_Uni/22_23/pap/Deep-Neural-Networks-for-Video-Classification/notebooks/../models/1/model_round_2.h5\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.72514 to 0.72790, saving model to /home/tiesbarendse/Documents/00_Uni/22_23/pap/Deep-Neural-Networks-for-Video-Classification/notebooks/../models/1/model_round_2.h5\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.72790 to 0.73066, saving model to /home/tiesbarendse/Documents/00_Uni/22_23/pap/Deep-Neural-Networks-for-Video-Classification/notebooks/../models/1/model_round_2.h5\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.73066\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.73066 to 0.73204, saving model to /home/tiesbarendse/Documents/00_Uni/22_23/pap/Deep-Neural-Networks-for-Video-Classification/notebooks/../models/1/model_round_2.h5\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.73204 to 0.74309, saving model to /home/tiesbarendse/Documents/00_Uni/22_23/pap/Deep-Neural-Networks-for-Video-Classification/notebooks/../models/1/model_round_2.h5\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.74309\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.74309\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.74309\n",
      "Epoch 9: early stopping\n",
      "H2 {'loss': [0.03029787540435791, 0.03538694232702255, 0.029259655624628067, 0.02629666030406952, 0.01879751682281494, 0.019547080621123314, 0.01750994846224785, 0.013027082197368145, 0.012779880315065384], 'accuracy': [0.969613254070282, 0.9640883803367615, 0.9723756909370422, 0.9779005646705627, 0.9834254384040833, 0.988950252532959, 0.9834254384040833, 0.988950252532959, 0.988950252532959], 'val_loss': [1.495286464691162, 1.3795907497406006, 1.7353311777114868, 1.5807132720947266, 1.6841325759887695, 1.831984519958496, 1.6732430458068848, 1.7836613655090332, 1.998482346534729], 'val_accuracy': [0.7251381278038025, 0.7279005646705627, 0.730663001537323, 0.7265193462371826, 0.7320442199707031, 0.7430939078330994, 0.7361878156661987, 0.7375690340995789, 0.7375690340995789]}\n",
      "stopped_epoch2 6\n",
      "9\n",
      "0.7361878156661987\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.73619, saving model to /home/tiesbarendse/Documents/00_Uni/22_23/pap/Deep-Neural-Networks-for-Video-Classification/notebooks/../models/1/model_round_3.h5\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.73619 to 0.73757, saving model to /home/tiesbarendse/Documents/00_Uni/22_23/pap/Deep-Neural-Networks-for-Video-Classification/notebooks/../models/1/model_round_3.h5\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.73757 to 0.73895, saving model to /home/tiesbarendse/Documents/00_Uni/22_23/pap/Deep-Neural-Networks-for-Video-Classification/notebooks/../models/1/model_round_3.h5\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.73895\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.73895\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.73895\n",
      "Epoch 6: early stopping\n",
      "H3 {'loss': [0.015183947049081326, 0.019510410726070404, 0.013066085986793041, 0.015030477195978165, 0.010670717805624008, 0.011813660152256489], 'accuracy': [0.9834254384040833, 0.9834254384040833, 0.988950252532959, 0.9861878156661987, 0.9944751262664795, 0.9944751262664795], 'val_loss': [1.9677643775939941, 1.982891321182251, 1.961715817451477, 1.9370166063308716, 1.918041467666626, 1.911628007888794], 'val_accuracy': [0.7361878156661987, 0.7375690340995789, 0.738950252532959, 0.738950252532959, 0.7375690340995789, 0.7375690340995789]}\n",
      "stopped_epoch3 3\n",
      "6\n",
      "0.738950252532959\n",
      "best fit round 3 0.738950252532959\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 40960)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               10486016  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,750,213\n",
      "Trainable params: 10,750,213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "  6/136 [>.............................] - ETA: 1s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-02 23:50:22.871188: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 711720960 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 1s 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-02 23:50:25,278 [MainThread  ] [INFO ]  {\n",
      "    \"architecture\": \"video_mlp_concat\",\n",
      "    \"batch_size\": 32,\n",
      "    \"bed\": true,\n",
      "    \"convolution_kernel_size\": 3,\n",
      "    \"custom_model_name\": \"ResNet50_test\",\n",
      "    \"data_total_rows_test\": 4344,\n",
      "    \"data_total_rows_train\": 362,\n",
      "    \"data_total_rows_valid\": 724,\n",
      "    \"dropout\": 0.2,\n",
      "    \"fit_best_round\": 3,\n",
      "    \"fit_dt_test_duration_seconds\": \"1\",\n",
      "    \"fit_dt_test_end\": \"2023-04-02 23:50:24\",\n",
      "    \"fit_dt_test_start\": \"2023-04-02 23:50:22\",\n",
      "    \"fit_dt_train_duration_seconds\": \"27\",\n",
      "    \"fit_dt_train_end\": \"2023-04-02 23:50:22\",\n",
      "    \"fit_dt_train_start\": \"2023-04-02 23:49:55\",\n",
      "    \"fit_num_epochs\": 20,\n",
      "    \"fit_stopped_epoch1\": 8,\n",
      "    \"fit_stopped_epoch2\": 6,\n",
      "    \"fit_stopped_epoch3\": 3,\n",
      "    \"fit_test_acc\": 0.7651933701657458,\n",
      "    \"fit_train_acc\": 0.9861878156661987,\n",
      "    \"fit_train_loss\": 0.015030477195978165,\n",
      "    \"fit_val_acc\": 0.738950252532959,\n",
      "    \"fit_val_loss\": 1.9370166063308716,\n",
      "    \"frame_size\": [\n",
      "        224,\n",
      "        224\n",
      "    ],\n",
      "    \"layer_1_size\": 256,\n",
      "    \"layer_2_size\": 512,\n",
      "    \"layer_3_size\": 256,\n",
      "    \"model_id\": 1,\n",
      "    \"model_param_count\": 10750213,\n",
      "    \"model_weights_path\": null,\n",
      "    \"num_features\": 2048,\n",
      "    \"path_model\": \"/home/tiesbarendse/Documents/00_Uni/22_23/pap/Deep-Neural-Networks-for-Video-Classification/notebooks/../models/1/\",\n",
      "    \"pooling\": \"max\",\n",
      "    \"pretrained_model_name\": \"resnet50\",\n",
      "    \"sequence_length\": 20,\n",
      "    \"sequence_model\": \"LSTM\",\n",
      "    \"sequence_model_layers\": 2,\n",
      "    \"verbose\": true\n",
      "}\n",
      "2023-04-02 23:50:25,285 [MainThread  ] [INFO ]  model 1 test acc: 0.7651933701657458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_id': 1, 'architecture': 'video_mlp_concat', 'sequence_length': 20, 'frame_size': (224, 224), 'pretrained_model_name': 'resnet50', 'custom_model_name': 'ResNet50_test', 'pooling': 'max', 'sequence_model': 'LSTM', 'sequence_model_layers': 2, 'layer_1_size': 256, 'layer_2_size': 512, 'layer_3_size': 256, 'dropout': 0.2, 'convolution_kernel_size': 3, 'model_weights_path': None, 'batch_size': 32, 'bed': True, 'verbose': True, 'num_features': 2048, 'path_model': '/home/tiesbarendse/Documents/00_Uni/22_23/pap/Deep-Neural-Networks-for-Video-Classification/notebooks/../models/1/', 'data_total_rows_train': 362, 'data_total_rows_valid': 724, 'data_total_rows_test': 4344, 'model_param_count': 10750213, 'fit_dt_train_start': '2023-04-02 23:49:55', 'fit_dt_train_end': '2023-04-02 23:50:22', 'fit_dt_train_duration_seconds': '27', 'fit_stopped_epoch1': 8, 'fit_stopped_epoch2': 6, 'fit_stopped_epoch3': 3, 'fit_num_epochs': 20, 'fit_val_acc': 0.738950252532959, 'fit_train_acc': 0.9861878156661987, 'fit_val_loss': 1.9370166063308716, 'fit_train_loss': 0.015030477195978165, 'fit_best_round': 3, 'fit_dt_test_start': '2023-04-02 23:50:22', 'fit_dt_test_end': '2023-04-02 23:50:24', 'fit_dt_test_duration_seconds': '1', 'fit_test_acc': 0.7651933701657458}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "architecture.train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f9fd7e5cd0ee174f389b9d7efcf5e4e8d178ce255061fb4046125c712618101"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
