{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:14:17.264442Z",
     "start_time": "2020-05-14T17:14:17.250090Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# whether to log each feature and sequence status\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:14:18.583218Z",
     "start_time": "2020-05-14T17:14:17.657048Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "from shutil import rmtree\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:14:18.993048Z",
     "start_time": "2020-05-14T17:14:18.988996Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__file__ = os.path.abspath('helper_precomputing_CNN_features.ipynb')\n",
    "DNN_lib_path = Path(__file__).parents[1].__str__()\n",
    "path_data = DNN_lib_path + '/data_cnn_ts_3d/'\n",
    "path_cache = DNN_lib_path + '/cache/'\n",
    "\n",
    "custom_model_name = 'ResNet50_test'\n",
    "path_features = path_cache + '/features/' + custom_model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:14:19.880582Z",
     "start_time": "2020-05-14T17:14:19.867630Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup logging\n",
    "# any explicit log messages or uncaught errors to stdout and file /logs.log\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(f\"{DNN_lib_path}/logs_{custom_model_name}_training.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "# init logger\n",
    "logger = logging.getLogger()\n",
    "# make logger aware of any uncaught exceptions\n",
    "def handle_exception(exc_type, exc_value, exc_traceback):\n",
    "    if issubclass(exc_type, KeyboardInterrupt):\n",
    "        sys.__excepthook__(exc_type, exc_value, exc_traceback)\n",
    "        return\n",
    "\n",
    "    logger.error(\"Uncaught exception\", exc_info=(exc_type, exc_value, exc_traceback))\n",
    "sys.excepthook = handle_exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:14:24.947193Z",
     "start_time": "2020-05-14T17:14:20.448078Z"
    }
   },
   "outputs": [],
   "source": [
    "from deepvideoclassification.architectures import Architecture\n",
    "from deepvideoclassification.data import Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>frame</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3</td>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3-0000.npy</td>\n",
       "      <td>pre-deboarding</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3</td>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3-0001.npy</td>\n",
       "      <td>pre-deboarding</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3</td>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3-0002.npy</td>\n",
       "      <td>pre-deboarding</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3</td>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3-0003.npy</td>\n",
       "      <td>pre-deboarding</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3</td>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3-0004.npy</td>\n",
       "      <td>pre-deboarding</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4</td>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4-0195.npy</td>\n",
       "      <td>post-boarding</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4</td>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4-0196.npy</td>\n",
       "      <td>post-boarding</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4</td>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4-0197.npy</td>\n",
       "      <td>post-boarding</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4</td>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4-0198.npy</td>\n",
       "      <td>post-boarding</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4</td>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4-0199.npy</td>\n",
       "      <td>post-boarding</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                video  \\\n",
       "0     trajs_2017-03-08_Ut_3064_door_3   \n",
       "1     trajs_2017-03-08_Ut_3064_door_3   \n",
       "2     trajs_2017-03-08_Ut_3064_door_3   \n",
       "3     trajs_2017-03-08_Ut_3064_door_3   \n",
       "4     trajs_2017-03-08_Ut_3064_door_3   \n",
       "...                               ...   \n",
       "1995  trajs_2018-05-21_Ut_3072_door_4   \n",
       "1996  trajs_2018-05-21_Ut_3072_door_4   \n",
       "1997  trajs_2018-05-21_Ut_3072_door_4   \n",
       "1998  trajs_2018-05-21_Ut_3072_door_4   \n",
       "1999  trajs_2018-05-21_Ut_3072_door_4   \n",
       "\n",
       "                                         frame           label  split  \n",
       "0     trajs_2017-03-08_Ut_3064_door_3-0000.npy  pre-deboarding  train  \n",
       "1     trajs_2017-03-08_Ut_3064_door_3-0001.npy  pre-deboarding  train  \n",
       "2     trajs_2017-03-08_Ut_3064_door_3-0002.npy  pre-deboarding  train  \n",
       "3     trajs_2017-03-08_Ut_3064_door_3-0003.npy  pre-deboarding  train  \n",
       "4     trajs_2017-03-08_Ut_3064_door_3-0004.npy  pre-deboarding  train  \n",
       "...                                        ...             ...    ...  \n",
       "1995  trajs_2018-05-21_Ut_3072_door_4-0195.npy   post-boarding  train  \n",
       "1996  trajs_2018-05-21_Ut_3072_door_4-0196.npy   post-boarding  train  \n",
       "1997  trajs_2018-05-21_Ut_3072_door_4-0197.npy   post-boarding  train  \n",
       "1998  trajs_2018-05-21_Ut_3072_door_4-0198.npy   post-boarding  train  \n",
       "1999  trajs_2018-05-21_Ut_3072_door_4-0199.npy   post-boarding  train  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df = pd.read_csv(path_data + 'labels.csv', usecols=['video','frame','label','split']).sort_values(['video', 'frame'])\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>frame</th>\n",
       "      <th>split</th>\n",
       "      <th>label_boarding</th>\n",
       "      <th>label_deboarding</th>\n",
       "      <th>label_phase-change</th>\n",
       "      <th>label_post-boarding</th>\n",
       "      <th>label_pre-deboarding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3</td>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3-0000.npy</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3</td>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3-0001.npy</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3</td>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3-0002.npy</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3</td>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3-0003.npy</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3</td>\n",
       "      <td>trajs_2017-03-08_Ut_3064_door_3-0004.npy</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4</td>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4-0195.npy</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4</td>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4-0196.npy</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4</td>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4-0197.npy</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4</td>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4-0198.npy</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4</td>\n",
       "      <td>trajs_2018-05-21_Ut_3072_door_4-0199.npy</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                video  \\\n",
       "0     trajs_2017-03-08_Ut_3064_door_3   \n",
       "1     trajs_2017-03-08_Ut_3064_door_3   \n",
       "2     trajs_2017-03-08_Ut_3064_door_3   \n",
       "3     trajs_2017-03-08_Ut_3064_door_3   \n",
       "4     trajs_2017-03-08_Ut_3064_door_3   \n",
       "...                               ...   \n",
       "1995  trajs_2018-05-21_Ut_3072_door_4   \n",
       "1996  trajs_2018-05-21_Ut_3072_door_4   \n",
       "1997  trajs_2018-05-21_Ut_3072_door_4   \n",
       "1998  trajs_2018-05-21_Ut_3072_door_4   \n",
       "1999  trajs_2018-05-21_Ut_3072_door_4   \n",
       "\n",
       "                                         frame  split  label_boarding  \\\n",
       "0     trajs_2017-03-08_Ut_3064_door_3-0000.npy  train               0   \n",
       "1     trajs_2017-03-08_Ut_3064_door_3-0001.npy  train               0   \n",
       "2     trajs_2017-03-08_Ut_3064_door_3-0002.npy  train               0   \n",
       "3     trajs_2017-03-08_Ut_3064_door_3-0003.npy  train               0   \n",
       "4     trajs_2017-03-08_Ut_3064_door_3-0004.npy  train               0   \n",
       "...                                        ...    ...             ...   \n",
       "1995  trajs_2018-05-21_Ut_3072_door_4-0195.npy  train               0   \n",
       "1996  trajs_2018-05-21_Ut_3072_door_4-0196.npy  train               0   \n",
       "1997  trajs_2018-05-21_Ut_3072_door_4-0197.npy  train               0   \n",
       "1998  trajs_2018-05-21_Ut_3072_door_4-0198.npy  train               0   \n",
       "1999  trajs_2018-05-21_Ut_3072_door_4-0199.npy  train               0   \n",
       "\n",
       "      label_deboarding  label_phase-change  label_post-boarding  \\\n",
       "0                    0                   0                    0   \n",
       "1                    0                   0                    0   \n",
       "2                    0                   0                    0   \n",
       "3                    0                   0                    0   \n",
       "4                    0                   0                    0   \n",
       "...                ...                 ...                  ...   \n",
       "1995                 0                   0                    1   \n",
       "1996                 0                   0                    1   \n",
       "1997                 0                   0                    1   \n",
       "1998                 0                   0                    1   \n",
       "1999                 0                   0                    1   \n",
       "\n",
       "      label_pre-deboarding  \n",
       "0                        1  \n",
       "1                        1  \n",
       "2                        1  \n",
       "3                        1  \n",
       "4                        1  \n",
       "...                    ...  \n",
       "1995                     0  \n",
       "1996                     0  \n",
       "1997                     0  \n",
       "1998                     0  \n",
       "1999                     0  \n",
       "\n",
       "[2000 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dummied = pd.get_dummies(label_df, columns=['label'])\n",
    "labels_dummied \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerization_dict = dict({\n",
    "    None : 4,\n",
    "    np.nan: 4, \n",
    "    'pre-deboarding': 0,\n",
    "    'deboarding': 1,\n",
    "    'phase-change': 2,\n",
    "    'boarding': 3,\n",
    "    'post-boarding': 0\n",
    "})\n",
    "\n",
    "class Data(object):\n",
    "\n",
    "    def __init__(self, sequence_length,\n",
    "                 return_CNN_features=True,\n",
    "                 pretrained_model_name=None,\n",
    "                 pooling=None,\n",
    "                 frame_size=None,\n",
    "                 custom_model_name=None,\n",
    "                 _bed=False,\n",
    "                 verbose=True,\n",
    "                 return_generator = False):\n",
    "        \"\"\"\n",
    "        Data object constructor\n",
    "        \n",
    "        \n",
    "        :sequence_length: number of frames in sequence to be returned by Data object\n",
    "        :return_CNN_features: whether to return precomputed features or return frames (or sequences of features/frames if sequence_length>1)\n",
    "\n",
    "        :return_features: if True then return features (or sequences of feature) from pretrained model, if False then return frames (or sequences of frames)        \n",
    "        :pretrained_model_name: name of pretrained model (or None if not using pretrained model e.g. for 3D-CNN)\n",
    "        :pooling: name of pooling variant (or None if not using pretrained model e.g. for 3D-CNN)\n",
    "        :frame_size: size that frames are resized to (this is looked up for pretrained models)\n",
    "        :aug3mentation: whether to apply data augmentation (horizontal flips)\n",
    "        :oversampling: whether to apply oversampling to create class balance\n",
    "        \n",
    "        :model_weights_path: path to custom model weights if we want to load CNN model we've fine-tuned to produce features (e.g. for LRCNN)\n",
    "        :custom_model_name: custom output name to append to pretrained model name\n",
    "        \n",
    "        :return_generator: if True and sequence_length > 1 and return_CNN_features == False, then do not return dataset, instead construct h5 file with sequences for each split and return generator that samples from that (dataset of sequecne frames too big to load into memory)\n",
    "        :batch_size: size of batches that generator must return\n",
    "        \n",
    "        :verbose: whether to log details\n",
    "        \n",
    "        Notes: \n",
    "        * if pretrained_model_name != None and return_CNN_features=False then will first apply preprocessor to frames (or frame sequences)\n",
    "        * if return_generator = True and sequence_length > 1 and return_CNN_features == False, large h5 files will be created in cache before returning generator\n",
    "        \"\"\"\n",
    "\n",
    "        # required params\n",
    "        self.sequence_length = sequence_length\n",
    "        self.frame_size = frame_size\n",
    "\n",
    "        # optional params\n",
    "        self.pretrained_model_name = pretrained_model_name\n",
    "        self.pooling = pooling\n",
    "        self.return_CNN_features = return_CNN_features\n",
    "        self.custom_model_name = custom_model_name\n",
    "        self.return_generator = return_generator\n",
    "\n",
    "        self.bed = _bed\n",
    "        self.frame_size = frame_size\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.x_train = []\n",
    "        self.y_train = []\n",
    "        #\n",
    "        self.x_valid = []\n",
    "        self.y_valid = []\n",
    "        #\n",
    "        self.x_test = []\n",
    "        self.y_test = []\n",
    "\n",
    "        ################\n",
    "        ### Prepare data\n",
    "        ################\n",
    "\n",
    "        #Label loading\n",
    "        self.labels = pd.read_csv(path_data + 'labels.csv', usecols=['video','frame','label','split']).sort_values(['video', 'frame'])\n",
    "        \n",
    "        # get label columns list and build label map dict\n",
    "        label_columns = []\n",
    "        label_map = {}\n",
    "        label_map_idx = 0\n",
    "        for i, col in enumerate(labels_dummied.columns):\n",
    "            if col[:6] == 'label_':\n",
    "                label_columns.append(col)\n",
    "                label_map[label_map_idx] = col\n",
    "                label_map_idx+=1\n",
    "\n",
    "        self.label_map = label_map\n",
    "\n",
    "        # get video paths\n",
    "        self.path_videos = [f\"{path_data}/{video}\" for video in os.listdir(path_data) if os.path.isdir(f\"{path_data}/{video}\")]\n",
    "\n",
    "        # check that there is 1 frame file for each label file and raise error if they don't match\n",
    "        path_frames = []\n",
    "        for folder, subs, files in os.walk(path_data):\n",
    "            for filename in files:\n",
    "                if self.bed & (filename[-4:].lower() == '.npy'):\n",
    "                    path_frames.append(os.path.abspath(\n",
    "                        os.path.join(folder, filename)))\n",
    "\n",
    "                if filename[-4:].lower() == '.jpg' or filename[-4:].lower() == 'jpeg' or filename[-4:].lower() == '.png':\n",
    "                    path_frames.append(os.path.abspath(\n",
    "                        os.path.join(folder, filename)))\n",
    "\n",
    "        if len(path_frames) != len(self.labels):\n",
    "            error_msg = 'IMPORTANT ERROR: Number of frames ({}) in /data/ video folders needs to match number of labels ({}) in labels.csv - use notebooks/helper_check_frames_against_labels.ipynb to investigate... Note, only labels.csv and the frames you want to use (in video subfolders) should be in /data/'.format(\n",
    "                len(path_frames), len(self.labels))\n",
    "            logger.info(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        # pull number of classes from labels shape\n",
    "        self.num_classes = self.labels['label'].nunique()\n",
    "\n",
    "        # create dict mapping video to train/valid/test split assignment\n",
    "        video_splits = self.labels[['video', 'split']].drop_duplicates()\n",
    "        video_splits.set_index(\"video\", inplace=True)\n",
    "        video_splits = video_splits.to_dict()['split']\n",
    "        self.video_splits = video_splits\n",
    "\n",
    "        # precompute resized frames (won't recompute if already resized)\n",
    "        #resize_frames(self.frame_size, _bed = self.bed, _verbose = self.verbose)\n",
    "\n",
    "        ###################################\n",
    "        ### load features / build sequences\n",
    "        ###################################\n",
    "\n",
    "        # load features/frames from all videos and concat into big array for each of train, valid and test\n",
    "        assert self.return_CNN_features\n",
    "\n",
    "        if verbose:\n",
    "            logging.info(\n",
    "                \"Loading features sequence data into memory [may take a few minutes]\")\n",
    "\n",
    "        #####################\n",
    "        ### feature sequences\n",
    "        #####################\n",
    "\n",
    "        path_features = path_cache + 'features/' + self.custom_model_name\n",
    "        path_labels = path_cache + 'labels/'\n",
    "        \n",
    "        bes_names = [be for be in os.listdir(path_data) if os.path.isdir(f\"{path_data}/{be}\")]\n",
    "\n",
    "        # loop over all vids and load precomputed features into memory as sequences\n",
    "        for c, be_name in enumerate(bes_names):\n",
    "\n",
    "            path_be = f'{path_data}/{be_name}'\n",
    "\n",
    "            if verbose:\n",
    "                logging.info(\"Loading features sequence data into memory {}/{}\".format(c+1,len(path_be)))\n",
    "\n",
    "            ### create sequence: features\n",
    "            # load precomputed features\n",
    "            features = np.load(f\"{path_features}/{be_name}.npy\")\n",
    "            # build sequences\n",
    "            x = []\n",
    "            for i in range(self.sequence_length, len(features) + 1):\n",
    "                x.append(features[i-self.sequence_length:i])\n",
    "            x = np.array(x)\n",
    "            \n",
    "\n",
    "            # temp lists to store sequences\n",
    "            be_labels = self.labels[self.labels.video == be_name]\n",
    "            y = []\n",
    "            for i in range(self.sequence_length, len(be_labels) + 1):\n",
    "                label = be_labels.label.iloc[i-1]\n",
    "                if (label is None) or (label == np.nan):\n",
    "                    label = 'nan'\n",
    "                y.append(label)\n",
    "            y = np.array(list(map(numerization_dict.get, y)))\n",
    "            y = to_categorical(y, num_classes=5)\n",
    "\n",
    "            assert len(x) == len(y), f'Length of features ({len(x)}) does not match length of labels ({len(y)})'\n",
    "\n",
    "            ### build output\n",
    "            if self.video_splits[be_name] == \"train\":\n",
    "                self.x_train.append(x)\n",
    "                self.y_train.append(y)\n",
    "            if self.video_splits[be_name] == \"valid\":\n",
    "                self.x_valid.append(x)\n",
    "                self.y_valid.append(y)\n",
    "            if self.video_splits[be_name] == \"test\":\n",
    "                self.x_test.append(x)\n",
    "                self.y_test.append(y)\n",
    "\n",
    "        #################################\n",
    "        ### get file paths for each split\n",
    "        #################################\n",
    "        #\n",
    "        # Note: only makes sense for sequence_length = 1\n",
    "\n",
    "        # get file paths: train\n",
    "        dflab = self.labels[self.labels['split'] == 'train']\n",
    "        self.paths_train = list(\n",
    "            path_data + dflab['video'] + \"/\" + dflab['frame'])\n",
    "\n",
    "        # get file paths: valid\n",
    "        dflab = self.labels[self.labels['split'] == 'valid']\n",
    "        self.paths_valid = list(\n",
    "            path_data + dflab['video'] + \"/\" + dflab['frame'])\n",
    "\n",
    "        # get file paths: test\n",
    "        dflab = self.labels[self.labels['split'] == 'test']\n",
    "        self.paths_test = list(\n",
    "            path_data + dflab['video'] + \"/\" + dflab['frame'])\n",
    "\n",
    "        #################################################\n",
    "        ### reshape list outputs (if not using generator)\n",
    "        #################################################\n",
    "\n",
    "        ## e.g. (9846, 224, 224, 3) for frames [return_CNN_features=True]\n",
    "        ## or  (9846, 512) for features [return_CNN_features=False]\n",
    "        self.x_train = np.concatenate(self.x_train, axis=0)\n",
    "        self.y_train = np.concatenate(self.y_train, axis=0)\n",
    "        self.x_valid = np.concatenate(self.x_valid, axis=0)\n",
    "        self.y_valid = np.concatenate(self.y_valid, axis=0)\n",
    "        self.x_test = np.concatenate(self.x_test, axis=0)\n",
    "        self.y_test = np.concatenate(self.y_test, axis=0)\n",
    "\n",
    "        self.total_rows_train = len(self.x_train)\n",
    "        self.total_rows_valid = len(self.x_valid)\n",
    "        self.total_rows_test = len(self.x_test)\n",
    "\n",
    "        # shuffle train and validation set\n",
    "        self.x_train, self.y_train = shuffle(self.x_train, self.y_train)\n",
    "        self.x_valid, self.y_valid = shuffle(self.x_valid, self.y_valid)\n",
    "\n",
    "        # update progress\n",
    "        if self.verbose:\n",
    "            print(\"Done initializing data with #samples: train={}, valid={}, test={}\".format(\n",
    "                self.total_rows_train, self.total_rows_valid, self.total_rows_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = {\n",
    "    'architecture': 'video_lrcnn_frozen',\n",
    "    'dropout': 0.2,\n",
    "    'layer_1_size': 256,\n",
    "    'layer_2_size': 512,\n",
    "    'layer_3_size': 256,\n",
    "    'model_id': 1,\n",
    "    'pooling': 'max',\n",
    "    'pretrained_model_name': 'resnet50',\n",
    "    'custom_model_name': custom_model_name,\n",
    "    'path_features': f'/cache/{custom_model_name}',\n",
    "    'sequence_length': 10,\n",
    "    'sequence_model': \"LSTM\",\n",
    "    'sequence_model_layers': 2,\n",
    "    'frame_size': (32, 32)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T17:15:28.835943Z",
     "start_time": "2020-05-14T17:14:48.938251Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-02 18:49:49,030 [MainThread  ] [INFO ]  Loading features sequence data into memory [may take a few minutes]\n",
      "2023-04-02 18:49:49,034 [MainThread  ] [INFO ]  Loading features sequence data into memory 1/145\n",
      "2023-04-02 18:49:49,056 [MainThread  ] [INFO ]  Loading features sequence data into memory 2/145\n",
      "2023-04-02 18:49:49,079 [MainThread  ] [INFO ]  Loading features sequence data into memory 3/145\n",
      "2023-04-02 18:49:49,103 [MainThread  ] [INFO ]  Loading features sequence data into memory 4/145\n",
      "2023-04-02 18:49:49,132 [MainThread  ] [INFO ]  Loading features sequence data into memory 5/147\n",
      "2023-04-02 18:49:49,158 [MainThread  ] [INFO ]  Loading features sequence data into memory 6/145\n",
      "2023-04-02 18:49:49,178 [MainThread  ] [INFO ]  Loading features sequence data into memory 7/144\n",
      "2023-04-02 18:49:49,199 [MainThread  ] [INFO ]  Loading features sequence data into memory 8/145\n",
      "2023-04-02 18:49:49,222 [MainThread  ] [INFO ]  Loading features sequence data into memory 9/145\n",
      "2023-04-02 18:49:49,245 [MainThread  ] [INFO ]  Loading features sequence data into memory 10/145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done initializing data with #samples: train=1337, valid=382, test=191\n"
     ]
    }
   ],
   "source": [
    "be_data = Data(\n",
    "    sequence_length = experiment['sequence_length'],\n",
    "    return_CNN_features = True, \n",
    "    pretrained_model_name = experiment['pretrained_model_name'],\n",
    "    pooling = experiment['pooling'],\n",
    "    frame_size = experiment['frame_size'],\n",
    "    custom_model_name= experiment['custom_model_name'],\n",
    "    return_generator = False,\n",
    "    _bed = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete existing results\n",
    "if os.path.exists(DNN_lib_path + '/models/' + str(experiment[\"model_id\"]) + '/results.json'):\n",
    "    rmtree(DNN_lib_path + '/models/' + str(experiment[\"model_id\"]) + '/')\n",
    "# create models folder if doesn't exist\n",
    "if not os.path.exists(DNN_lib_path + '/models/'):\n",
    "    os.makedirs(DNN_lib_path + '/models/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-02 18:49:49,996 [MainThread  ] [INFO ]  Model folder exists but no results found - potential error in previous model training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "{'architecture': 'video_lrcnn_frozen', 'dropout': 0.2, 'layer_1_size': 256, 'layer_2_size': 512, 'layer_3_size': 256, 'model_id': 1, 'pooling': 'max', 'pretrained_model_name': 'resnet50', 'custom_model_name': 'ResNet50_test', 'path_features': '/cache/ResNet50_test', 'sequence_length': 10, 'sequence_model': 'LSTM', 'sequence_model_layers': 2, 'frame_size': (32, 32)}\n"
     ]
    }
   ],
   "source": [
    "print(str(experiment[\"model_id\"]) + \"   \" + \"X\"*60)\n",
    "print(experiment)\n",
    "\n",
    "architecture = Architecture(model_id = experiment['model_id'], \n",
    "                            architecture = experiment['architecture'], \n",
    "                            sequence_length = experiment['sequence_length'], \n",
    "                            pretrained_model_name = experiment['pretrained_model_name'],\n",
    "                            custom_model_name = experiment['custom_model_name'],\n",
    "                            pooling = experiment['pooling'],\n",
    "                            sequence_model = experiment['sequence_model'],\n",
    "                            sequence_model_layers = experiment['sequence_model_layers'],\n",
    "                            layer_1_size = experiment['layer_1_size'],\n",
    "                            layer_2_size = experiment['layer_2_size'],\n",
    "                            layer_3_size = experiment['layer_3_size'],\n",
    "                            dropout = experiment['dropout'],\n",
    "                            _bed = True,\n",
    "                            verbose=True,\n",
    "                            data = be_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20191812\\.conda\\envs\\train_passengers\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2023-04-02 18:50:31,520 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:50:31,522 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:51:14,133 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:51:14,136 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:51:45,542 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:51:45,545 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:52:03,013 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:52:03,016 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:52:20,009 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:52:20,013 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:52:37,432 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:52:37,434 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:52:57,631 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:52:57,633 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:53:22,124 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:53:22,126 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:53:38,743 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:53:38,746 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:53:55,683 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:53:55,685 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:54:11,261 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:54:11,263 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:54:26,781 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:54:26,783 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:54:43,484 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:54:43,486 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:55:00,042 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:55:00,045 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:55:15,453 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:55:15,455 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:55:30,635 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:55:30,637 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:55:46,907 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:55:46,909 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:56:04,203 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:56:04,205 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:56:20,753 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:56:20,756 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n",
      "2023-04-02 18:56:36,742 [MainThread  ] [WARNI]  Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "2023-04-02 18:56:36,743 [MainThread  ] [WARNI]  Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1 {'loss': [0.324916809797287, 0.2001872956752777, 0.19023577868938446, 0.18108265101909637, 0.167916938662529, 0.15677984058856964, 0.1397092193365097, 0.1300416886806488, 0.13119205832481384, 0.12097019702196121, 0.1122579500079155, 0.10894537717103958, 0.10003870725631714, 0.10599514842033386, 0.10137270390987396, 0.09289596229791641, 0.10309750586748123, 0.09732480347156525, 0.09447501599788666, 0.09922315925359726], 'accuracy': [0.6881077289581299, 0.7314884066581726, 0.7636499404907227, 0.7816005945205688, 0.8002991676330566, 0.8092744946479797, 0.8339565992355347, 0.8526551723480225, 0.8466716408729553, 0.8631263971328735, 0.8683620095252991, 0.8803290724754333, 0.8997756242752075, 0.8743455410003662, 0.8922961950302124, 0.89603590965271, 0.8758414387702942, 0.8922961950302124, 0.8937920928001404, 0.8997756242752075], 'val_loss': [0.3505215048789978, 0.3440079092979431, 0.35014867782592773, 0.34642085433006287, 0.3268750309944153, 0.32269346714019775, 0.34288015961647034, 0.34935808181762695, 0.2736261785030365, 0.3055781424045563, 0.26047948002815247, 0.2914051115512848, 0.25873151421546936, 0.3157159388065338, 0.30634546279907227, 0.2907792925834656, 0.3374309837818146, 0.24299116432666779, 0.33381667733192444, 0.4374341666698456], 'val_accuracy': [0.5130890011787415, 0.47905758023262024, 0.518324613571167, 0.5261780023574829, 0.5942408442497253, 0.6178010702133179, 0.6230366230010986, 0.5942408442497253, 0.6596858501434326, 0.6047120690345764, 0.6492146849632263, 0.6518324613571167, 0.6623036861419678, 0.6439790725708008, 0.6335078477859497, 0.5994764566421509, 0.6465968489646912, 0.6884816884994507, 0.6178010702133179, 0.5994764566421509]}\n",
      "stopped_epoch1 19\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m architecture\u001b[39m.\u001b[39;49mtrain_model()\n",
      "File \u001b[1;32mc:\\Users\\20191812\\Documents\\00_Uni\\Y4\\Q3\\3QAUS0\\pap\\Deep-Neural-Networks-for-Video-Classification\\notebooks\\..\\deepvideoclassification\\architectures.py:850\u001b[0m, in \u001b[0;36mArchitecture.train_model\u001b[1;34m(self, epochs, patience)\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mH1\u001b[39m\u001b[39m'\u001b[39m, history1\u001b[39m.\u001b[39mhistory)\n\u001b[0;32m    849\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mstopped_epoch1\u001b[39m\u001b[39m'\u001b[39m,stopped_epoch1)\n\u001b[1;32m--> 850\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(history1\u001b[39m.\u001b[39;49mhistory[\u001b[39m'\u001b[39;49m\u001b[39mval_acc\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n\u001b[0;32m    851\u001b[0m \u001b[39mprint\u001b[39m(history1\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_acc\u001b[39m\u001b[39m'\u001b[39m][stopped_epoch1])\n\u001b[0;32m    853\u001b[0m \u001b[39m# update best fit round (only 1 round done so this is best so far)\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_acc'"
     ]
    }
   ],
   "source": [
    "architecture.train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
